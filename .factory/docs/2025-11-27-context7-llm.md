Сейчас в "LLM Conversation" ты видишь только диалог с моделью через cliproxy, а context7 там не появляется, потому что с ним нет чата: бекенд делает один технический вызов (MCP tool / HTTP‑запрос) на каждый `/docs/search` и сразу получает сниппеты.

Если хочешь видеть и эту часть, логика будет такой:
- В `server.py` помечаем, откуда пришли сниппеты: MCP (`source: "context7_mcp"`), HTTP (`"context7_http"`) или локальные доки (`"local"`), и добавляем эту метку (и, при желании, `library_id`/`topic`) в `results`.
- На фронте в `fetchDocsContext` сохраняем не только текст `docsContext`, но и метаданные по каждому сниппету (source, file), чтобы знать, что именно вернул context7.
- Добавляем в UI отдельный блок, например `Docs / Context7`, где показываем для текущего запроса:
  - какую строку отправили в `/docs/search`;
  - откуда реально пришёл ответ (context7 MCP / HTTP / local);
  - сырые сниппеты (по сути то, что уже подмешивается в промпт, но с явной пометкой `[context7]` или `[local]`).
- Диалог с LLM остаётся отдельным блоком (как сейчас), а блок context7 показывает все запросы/ответы бекенда к context7 в терминах "q → snippets".

Так будет чётко видно: 1) какой текст мы отправили в docs‑поиск, 2) действительно ли попали в context7 или сработал локальный fallback, 3) какие фрагменты документации потом попали в промпт к LLM.